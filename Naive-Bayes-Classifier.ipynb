{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile,join\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import random\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集我们使用的是 IMDB 数据集。这个数据集包含 25000 条电影数据，其中 12500 条正向数据，12500 条负向数据。这些数据都是存储在一个文本文件中，首先我们需要做的就是去解析这个文件。正向数据包含在一个文件中，负向数据包含在另一个文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveFiles = [\"d:/input_data/aclImdb_v1/aclImdb/train/pos/\" + f for f in listdir(\"d:/input_data/aclImdb_v1/aclImdb/train/pos/\") if isfile(join(\"d:/input_data/aclImdb_v1/aclImdb/train/pos/\",f))]\n",
    "negativeFiles = [\"d:/input_data/aclImdb_v1/aclImdb/train/neg/\" + f for f in listdir(\"d:/input_data/aclImdb_v1/aclImdb/train/neg/\") if isfile(join(\"d:/input_data/aclImdb_v1/aclImdb/train/neg/\",f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf,'r',encoding = \"utf-8\") as f:\n",
    "        line = f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)\n",
    "print(\"Positive files finished!\")\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf,\"r\",encoding = \"utf-8\") as f:\n",
    "        line = f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)\n",
    "print(\"Negative files finished!\")\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print(\"The total number of line is \",numFiles)\n",
    "print(\"the total number of words in the files is\",sum(numWords))\n",
    "print(\"the average number of words in the files is\",sum(numWords)/len(numWords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据清洗\n",
    "# 停用词\n",
    "stopwords = {}.fromkeys([ line.rstrip() for line in open(\"d:/input_data/word2vec/kaggle-word2vec/stopwords.txt\")])\n",
    "eng_stopwords = set(stopwords)\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text,'html.parser').get_text()\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
    "    words = text.lower().split()\n",
    "    words = [ w for w in words if w not in eng_stopwords]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#英文分词模块\n",
    "tokenizer = nltk.data.load(\"../nltk_data/tokenizers/punkt/english.pickle\")\n",
    "# 分句方法\n",
    "def split_sentences(words):\n",
    "    raw_sentences = tokenizer.tokenize(words.strip())\n",
    "    sentences = [clean_text(s) for s in raw_sentences if s]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeWordsSet(words_file):\n",
    "    words_set = set()\n",
    "    with open(words_file,'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            cleanLine = clean_text(line)\n",
    "            word = cleanLine.strip()\n",
    "            if len(word)>0 and word not in words_set:\n",
    "                words_set.add(word)\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextProcessing(floder_path,test_size = 0.2):\n",
    "    folder_list = os.listdir(folder_path)[1:3]\n",
    "    data_list = []\n",
    "    class_list = []\n",
    "    \n",
    "    # 类间循环\n",
    "    for folder in folder_list:\n",
    "        if folder == 'pos':\n",
    "            flag = 1\n",
    "        elif folder == 'neg':\n",
    "            flag = 0\n",
    "        new_folder_path = os.path.join(folder_path,folder)\n",
    "        files = os.listdir(new_folder_path)\n",
    "        \n",
    "        # 类内循环\n",
    "        for file in files:\n",
    "            # 读取文件\n",
    "            with open(os.path.join(new_folder_path,file),'r',encoding=\"utf-8\") as fp:\n",
    "                line = fp.readline()\n",
    "                line = clean_text(line)\n",
    "            data_list.append(line)\n",
    "            class_list.append(flag)\n",
    "            \n",
    "    data_class_list = list(zip(data_list, class_list))\n",
    "    np.random.shuffle(data_class_list)\n",
    "    index = int(len(data_class_list)*test_size)+1\n",
    "    train_list = data_class_list[index:]\n",
    "    test_list = data_class_list[:index]\n",
    "    train_data_list,train_class_list = zip(*train_list)\n",
    "    test_data_list,test_class_list = zip(*test_list)\n",
    "    \n",
    "    # 统计词频放入all_words_dict\n",
    "    all_words_dict = {}\n",
    "    for word_list in train_data_list:\n",
    "        for word in word_list:\n",
    "            if word in all_words_dict:\n",
    "                all_words_dict[word] += 1\n",
    "            else:\n",
    "                all_words_dict[word] = 1\n",
    "    \n",
    "    # key函数利用词频进行降序排序\n",
    "    # 内建函数sorted参数需为list\n",
    "    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f:f[1], reverse=True) # 内建函数sorted参数需为list\n",
    "    all_words_list = list(zip(*all_words_tuple_list))[0]\n",
    "    \n",
    "    return all_words_dict,all_words_list,train_data_list,test_data_list,train_class_list,test_class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_dict(all_words_list,deleteN,N):\n",
    "    # 特征词\n",
    "    feature_words = []\n",
    "    n = 1\n",
    "    for t in range(deleteN,len(all_words_list),1):\n",
    "        if n>N:\n",
    "            break\n",
    "        if 3<len(all_words_list[t])<20:\n",
    "            feature_words.append(all_words_list[t])\n",
    "            n+= 1\n",
    "    return feature_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextFeatures(train_data_list,test_data_list,feature_words,flag = 'nltk'):\n",
    "    def text_features(text,feature_words):\n",
    "        text_words = set(text)\n",
    "        if flag == 'nltk':\n",
    "            # nltk特征 dict\n",
    "            features = {word:1 if word in text_words else 0 for word in feature_words}\n",
    "        elif flag == 'sklearn':\n",
    "            features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        else:\n",
    "            features = []\n",
    "        return features\n",
    "    \n",
    "    train_feature_list = [text_features(text,feature_words) for text in train_data_list]\n",
    "    test_feature_list = [text_features(text,feature_words) for text in test_data_list]\n",
    "    return train_feature_list,test_feature_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextClassifier(train_feature_list,test_feature_list,train_class_list,test_class_list, flag='nltk'):\n",
    "    if flag == 'nltk':\n",
    "        # nltk 分类器\n",
    "        train_flist = zip(train_feature_list,train_class_list)\n",
    "        test_flist = zip(test_feature_list,test_class_list)\n",
    "        classifier = nltk.classify.NaiveBayesClassifier.train(train_flist)\n",
    "        test_accuracy = nltk.classify.accuracy(classifier,test_flist)\n",
    "    elif flag == 'sklearn':\n",
    "        # sklearn 分类器\n",
    "        classifier = MultinomialNB().fit(train_feature_list,train_class_list)\n",
    "        test_accuracy = classifier.score(test_feature_list,test_class_list)\n",
    "    else:\n",
    "        test_accuracy = []\n",
    "    return test_accuracy,classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 文本预处理\n",
    "folder_path = \"d:/input_data/aclImdb_v1/aclImdb/train/\"\n",
    "all_words_dict,all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = TextProcessing(folder_path, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率：0.8370325934813038\n"
     ]
    }
   ],
   "source": [
    "sentiment_to_num = {\"pos\":1,\"neg\":0}\n",
    "flag = 'sklearn'\n",
    "# test_accuracy_list = []\n",
    "# deleteNS = range(0,100,20)\n",
    "# i = 1\n",
    "# for deleteN in deleteNS:\n",
    "#     feature_words = words_dict(all_words_list,deleteN,N)\n",
    "#     train_feature_list,test_feature_list = TextFeatures(train_data_list,test_data_list,feature_words, flag)\n",
    "#     test_accuracy,classifier = TextClassifier(train_feature_list,test_feature_list,train_class_list, test_class_list, flag)\n",
    "#     test_accuracy_list.append(test_accuracy)\n",
    "#     print(\"第{}次训练完毕，准确率：{}\".format(i,test_accuracy))\n",
    "#     i += 1\n",
    "N=2000\n",
    "deleteN = 20\n",
    "feature_words = words_dict(all_words_list,deleteN,N)\n",
    "train_feature_list,test_feature_list = TextFeatures(train_data_list,test_data_list,feature_words, flag)\n",
    "test_accuracy,classifier = TextClassifier(train_feature_list,test_feature_list,train_class_list, test_class_list, flag)\n",
    "test_accuracy_list.append(test_accuracy)\n",
    "print(\"准确率：{}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为单个的评论创建文本特征\n",
    "def testTextFeatures(comment_data,feature_words,flag = 'nltk'):\n",
    "    data = np.array(comment_data).reshape(1,-1)\n",
    "    def text_features(text,feature_words):\n",
    "        text_words = set(text)\n",
    "        if flag == 'nltk':\n",
    "            # nltk特征 dict\n",
    "            features = {word:1 if word in text_words else 0 for word in feature_words}\n",
    "        elif flag == 'sklearn':\n",
    "            features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        else:\n",
    "            features = []\n",
    "        return features\n",
    "    \n",
    "    data_feature = [text_features(text,feature_words) for text in data]\n",
    "    return data_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试单独的评论\n",
    "def test_comment(comment_data,classifier,features,flag='sklean'):\n",
    "    comment_data = clean_text(comment_data)\n",
    "    data_feature = testTextFeatures(comment_data,feature_words, flag)\n",
    "    pred = classifier.predict(data_feature)\n",
    "    result = num_to_sentiment[int(pred)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "comment1 = 'the movie is very prefect,I love it very much,I will support it forever!'\n",
    "comment2 = 'the movie is very terrible,I do not like it very much,I do not see it never!'\n",
    "res = test_comment(comment2,classifier,feature_words,flag)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
